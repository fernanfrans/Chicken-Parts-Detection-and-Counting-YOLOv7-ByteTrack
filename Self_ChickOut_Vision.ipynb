{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd2a3961-caf8-4573-8dfc-a7fb3f0af38c",
   "metadata": {},
   "source": [
    "# YoloV7 Tracking and Counting using Bytetrack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e17641-ae60-4898-9878-46ae9146384c",
   "metadata": {},
   "source": [
    "## Import Libraries and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb0ccd0-7534-4f2d-a56d-4002aadb1a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import Dict, Tuple, List\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "import time\n",
    "\n",
    "# YOLOv7 specific imports\n",
    "from models.experimental import attempt_load\n",
    "from utils.general import check_img_size, non_max_suppression, scale_coords\n",
    "from utils.torch_utils import select_device\n",
    "\n",
    "# Bytetrack specific imports\n",
    "from yolox.tracker.byte_tracker import BYTETracker, STrack\n",
    "from onemetric.cv.utils.iou import box_iou_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007abd4d-b02a-406b-8d93-7ccd3fd63150",
   "metadata": {},
   "source": [
    "- cv2: OpenCV library for computer vision tasks.\n",
    "- torch: PyTorch library for machine learning.\n",
    "- np: NumPy library for numerical operations.\n",
    "- Sort: SORT tracker for object tracking.\n",
    "- logging: Python's logging module for logging events.\n",
    "- dataclass: To define configuration as a data class.\n",
    "- YOLOv7 specific imports for model loading and processing.\n",
    "- Bytetrack specific imports for tracking and counting box_iou_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec287eaa-aed4-4ff0-aeb1-b7f6de2b38e2",
   "metadata": {},
   "source": [
    "## Data Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64cc0de-ae9b-4b68-9e02-a6161d77cc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    MODEL_PATH: str = \"best_v7.pt\" # path to the model file\n",
    "    FRAME_WIDTH: int = 1280 # width of the frame\n",
    "    FRAME_HEIGHT: int = 720 # width of the frame\n",
    "    CONFIDENCE_THRESHOLD: float = 0.5 # confidence threshold for detection\n",
    "    IOU_THRESHOLD: float = 0.3 # Intersection over Union threshold for non-max suppression.\n",
    "    FONT_SCALE: float = 1 # Font scale for drawing text on frames.\n",
    "    FONT_THICKNESS: int = 2 # Font thickness for drawing text on frames.\n",
    "\n",
    "config = Config()\n",
    "\n",
    "@dataclass\n",
    "class BYTETrackerArgs:\n",
    "    track_thresh: float = 0.25 # Threshold for initiating new tracks.\n",
    "    track_buffer: int = 30 # Buffer size for track management.\n",
    "    match_thresh: float = 0.8 # Threshold for matching detections to existing tracks.\n",
    "    aspect_ratio_thresh: float = 3.0 # Aspect ratio threshold for filtering tracks.\n",
    "    min_box_area: float = 1.0 #  Minimum bounding box area for tracks.\n",
    "    mot20: bool = False # Flag for MOT20 dataset compatibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cdd4a1-b580-45a7-9226-9ced81a4fcfd",
   "metadata": {},
   "source": [
    "These classes define the parameters for object detection and tracking in our system. The Config class contains values used for object detection with YOLOv7, while the BYTETrackerArgs class holds parameters for the ByteTrack tracker. We've encapsulated these parameters within classes to enhance readability and facilitate easy modifications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63dfcc8-128b-4670-8f0b-0abc1be38c09",
   "metadata": {},
   "source": [
    "## Camera Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900624b7-0d3e-4e77-a622-aaa886cf9654",
   "metadata": {},
   "source": [
    "Initializes the video capture from a camera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94452722-2006-42f1-ab21-9a78135d3007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_camera(config: Config) -> cv2.VideoCapture:\n",
    "    cap = cv2.VideoCapture(0) #open the camera (check the index of the device)\n",
    "    if not cap.isOpened(): #check if the camera is not opened\n",
    "        raise RuntimeError(\"Error: Could not open video capture.\") # if it doesn't work it will raise a runtime error\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, config.FRAME_WIDTH) #Set the width of the frame with the given width in the config class.\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, config.FRAME_HEIGHT)#Set the height of the frame with the given height in the config class.\n",
    "    return cap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7a81a0-fd31-46e5-9905-390f965f6ef9",
   "metadata": {},
   "source": [
    "## Detections Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9c0d58-00d7-4db2-8deb-ea3d6d74217b",
   "metadata": {},
   "source": [
    "The Detections class is a custom data structure designed to store and manage the results of object detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da82d074-4160-404c-9d74-01abb9ddc2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Detections:\n",
    "    # Constructor method with three parameters: xyxy, confidence, and class_id. To initialize the attributes of an object when it is created. \n",
    "    def __init__(self, xyxy, confidence, class_id):\n",
    "        self.xyxy = xyxy\n",
    "        self.confidence = confidence\n",
    "        self.class_id = class_id\n",
    "        self.tracker_id = None\n",
    "\n",
    "    # filtering detections based on a given mask.\n",
    "    def filter(self, mask, inplace=False):\n",
    "        # Check if the filtering should be done in-place\n",
    "        if inplace:\n",
    "            # Update the current object's attributes based on the mask\n",
    "            self.xyxy = self.xyxy[mask]\n",
    "            self.confidence = self.confidence[mask]\n",
    "            self.class_id = self.class_id[mask]\n",
    "            # If tracker_id is not None, update it based on the mask\n",
    "            if self.tracker_id is not None:\n",
    "                self.tracker_id = self.tracker_id[mask]\n",
    "        else:\n",
    "            # Return a new Detections object with filtered attributes\n",
    "            return Detections(\n",
    "                self.xyxy[mask],\n",
    "                self.confidence[mask],\n",
    "                self.class_id[mask]\n",
    "            )\n",
    "\n",
    "    #  simplifies iteration over the Detections object by providing a straightforward way to access each detection's attributes in a loop.\n",
    "    # this make our life easier to access different attributes\n",
    "    def __iter__(self):\n",
    "        for xyxy, confidence, class_id, tracker_id in zip(\n",
    "            self.xyxy, self.confidence, self.class_id, self.tracker_id if self.tracker_id.size > 0 else [None] * len(self.xyxy)\n",
    "        ):\n",
    "            yield xyxy, confidence, class_id, tracker_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a024ca74-1a3e-416c-847c-70f34a48f627",
   "metadata": {},
   "source": [
    "## Object Detector Class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931b0c9b-85d6-4727-988b-9b1d62933045",
   "metadata": {},
   "source": [
    "Then we create an ObjectDetector class. First, we initialize instance variables, this will be use in different functions in the class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e336502a-ae09-4f35-ace7-af4cf67d1896",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectDetector:\n",
    "    # initialize the paramaters for object detection by accessing both data classes \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.device = select_device('') # Select the device (CPU or GPU) for computation\n",
    "        self.model = self._load_model() # Load the YOLOv7 model using load_model method\n",
    "        self.byte_tracker = BYTETracker(BYTETrackerArgs()) # Initialize the BYTE tracker\n",
    "        self.imgsz = check_img_size(640, s=self.model.stride.max()) # Check and set the input image size, ensuring it's compatible with the model's stride\n",
    "        # Correct handling of self.names assignment\n",
    "        self.names = self.model.module.names if hasattr(self.model, 'module') else self.model.names\n",
    "        \n",
    "        # Ensure self.names is a dictionary\n",
    "        if isinstance(self.names, list): # If self.names is a list, convert it to a dictionary with indices as keys\n",
    "            self.names = {i: name for i, name in enumerate(self.names)}\n",
    "        logger.info(f\"Input image size: {self.imgsz}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66e901a-eb68-42e5-bb03-fee229ac724f",
   "metadata": {},
   "source": [
    "### Loading the Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cab6177-3180-4978-ab1a-ce545ae20028",
   "metadata": {},
   "source": [
    "The first function of the ObjectDetector class is _load_model(). This will be use to load the best model that was train using the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35d9eca-aaa8-40ab-b97c-745e61379673",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _load_model(self) -> torch.nn.Module:\n",
    "        model = attempt_load(self.config.MODEL_PATH, map_location=self.device)\n",
    "        logger.info(f\"Model loaded. Number of classes: {len(model.names)}\")\n",
    "        logger.info(f\"Class names: {model.names}\")\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079b4471-608e-4960-b350-6eb241d70e02",
   "metadata": {},
   "source": [
    "## Processing Each Frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f631f0e2-e095-4bb7-966e-5d88ed2cf6b8",
   "metadata": {},
   "source": [
    "This function will detect and track objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c45adae-53fe-4bcc-8ce4-69a54599885e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ef process_frame(self, frame: np.ndarray) -> Tuple[np.ndarray, Dict[str, int], List[str]]:\n",
    "        # First, initialize class_counts dictionary that we will be use later for counting of the detected objects\n",
    "        class_counts = {}\n",
    "        chicken_parts = []\n",
    "        # dict mapping class_id to class_name\n",
    "        CLASS_NAMES_DICT = self.names\n",
    "        # class_ids of interest \n",
    "        CLASS_ID = list(CLASS_NAMES_DICT.keys())\n",
    "\n",
    "        # pre-process the frame so that it will be fit to the requirement of the model\n",
    "        img = cv2.resize(frame, (self.imgsz, self.imgsz)) # resize the frame\n",
    "        img = img.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
    "        img = np.ascontiguousarray(img) #convert to contiguous array\n",
    "        img = torch.from_numpy(img).to(self.device) # convert array to PyTorch tensor then move it to device\n",
    "        img = img.float() / 255.0 # normalize the tensor values to (1, 0)\n",
    "        if img.ndimension() == 3: # check the dimesion and add one to the first index\n",
    "            img = img.unsqueeze(0)\n",
    "\n",
    "        # initialize the starting time of detections\n",
    "        start_time = time.time()\n",
    "\n",
    "        # disables gradient calculation during inference so that it will save memory and computation time\n",
    "        with torch.no_grad():\n",
    "            pred = self.model(img, augment=False)[0] # predict using the model \n",
    "\n",
    "        # initialize the end time of the detections\n",
    "        end_time = time.time()\n",
    "        # get the inference time and convert it to milliseconds\n",
    "        inference_time = (end_time - start_time) * 1000\n",
    "\n",
    "        # filter out overlapping boxes\n",
    "        pred = non_max_suppression(pred, self.config.CONFIDENCE_THRESHOLD, self.config.IOU_THRESHOLD)\n",
    "\n",
    "        # Ensure all GPU operations are complete before proceeding\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "        if len(pred[0]) > 0: #check if there is detected\n",
    "            det = pred[0] # Extract the first element containing detections\n",
    "            det[:, :4] = scale_coords(img.shape[2:], det[:, :4], frame.shape).round() \n",
    "            #adjust the dimensions of the images to the frames and round it up so that it will ensures that the bounding box is in proper place \n",
    "            \n",
    "            # creating an instance of Detections class named detections\n",
    "            detections = Detections(\n",
    "                xyxy=det[:, :4].cpu().numpy(), # coordinates\n",
    "                confidence=det[:, 4].cpu().numpy(), # confidence value of the detected object\n",
    "                class_id=det[:, 5].cpu().numpy().astype(int) # class id of the detected object\n",
    "            )\n",
    "\n",
    "            # filtering out detections with unwanted classes\n",
    "            mask = np.array([class_id in CLASS_ID for class_id in detections.class_id], dtype=bool)\n",
    "            detections.filter(mask=mask, inplace=True)\n",
    "\n",
    "            \"\"\"\n",
    "            # A mask array created to check if there is new detected object \n",
    "            mask = np.array([tracker_id is not None for tracker_id in detections.tracker_id], dtype=bool)\n",
    "            \n",
    "            This can be constructed as shown below for easier readability: \n",
    "                mask_list = [] # Initialize an empty list to store the mask values\n",
    "                for class_id in detections.class_id: # Iterate over each class_id in detections.class_id\n",
    "                    if class_id in CLASS_ID: # Check if the current class_id is in CLASS_ID\n",
    "                        mask_list.append(True) # If it is, append True to the mask_list\n",
    "                    else:\n",
    "                        mask_list.append(False) # If it is not, append False to the mask_list\n",
    "                # Convert the mask_list to a NumPy array of boolean type\n",
    "                mask = np.array(mask_list, dtype=bool)\n",
    "            \"\"\"\n",
    "\n",
    "            # tracking detections\n",
    "            tracks = self.byte_tracker.update(\n",
    "                output_results=self.detections2boxes(detections=detections),\n",
    "                img_info=frame.shape,\n",
    "                img_size=frame.shape\n",
    "            )\n",
    "            #obtaining track id by using the method match_detections_with_tracks\n",
    "            tracker_id = self.match_detections_with_tracks(detections=detections, tracks=tracks)\n",
    "            detections.tracker_id = np.array(tracker_id)\n",
    "\n",
    "\n",
    "            # Annotating the frame. Format custom labels and draw bounding boxes\n",
    "            for xyxy, confidence, class_id, tracker_id in detections:\n",
    "                x1, y1, x2, y2 = xyxy.astype(int) # coordinates of the bounding box\n",
    "                class_name = CLASS_NAMES_DICT[class_id] # class name of the detected object \n",
    "                \n",
    "                class_counts[class_name] = class_counts.get(class_name, 0) + 1 # update the count of class_counts dict\n",
    "                chicken_parts.append(class_name) # append the class_name to chicken_parts list\n",
    "\n",
    "                color = self.get_color_for_class(class_id) # getting random color using the function get_color_for_class\n",
    "                label = f\"#{tracker_id} {class_name} {confidence:.2f}\" # a string for label (#1 Thigh 70)\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2) # create a rectangle using the coordinates of the detected object for bounding box\n",
    "                cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2) # put the label in the frame\n",
    "\n",
    "            # put text for the objects per class that is detected\n",
    "            for i, (class_name, count) in enumerate(class_counts.items()):\n",
    "                cv2.putText(frame, f\"{class_name}: {count}\", (10, 30 + i * 30), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, self.config.FONT_SCALE, (255, 255, 255), self.config.FONT_THICKNESS)\n",
    "\n",
    "        return frame, class_counts, chicken_parts, inference_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3050af12-5162-41de-a967-d0a55c6e54af",
   "metadata": {},
   "source": [
    "### Generating Colors for Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccd4365-147b-4aab-8720-ee56a784a3b8",
   "metadata": {},
   "source": [
    "This function is created for giving the classes different color from each other when they are detected and putting boundix box around it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3ac195-3779-4f5f-95d0-9c63cbe5e42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "        @staticmethod \n",
    "        def get_color_for_class(class_id: int) -> Tuple[int, int, int]:\n",
    "            np.random.seed(class_id)\n",
    "            return tuple(np.random.randint(0, 255, size=3).tolist()) # randomized 3 numbers for color (RGB) then convert it array to list and list to tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389c3fc3-dfe7-4753-8db7-92b669f97d68",
   "metadata": {},
   "source": [
    "## Tracking Utils\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34579e94-6aa4-4758-87ab-24b2005f36f8",
   "metadata": {},
   "source": [
    "We need to manually match the bounding boxes generated by our model with those produced by the tracker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b792b104-8c7a-4a60-a33f-5f0f88e1efdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "    @staticmethod\n",
    "    # converts Detections into format that can be consumed by match_detections_with_tracks function\n",
    "    def detections2boxes(detections: Detections) -> np.ndarray:\n",
    "        return np.hstack(( # horizontal stack\n",
    "            detections.xyxy,\n",
    "            detections.confidence[:, np.newaxis]\n",
    "        ))\n",
    "\n",
    "    @staticmethod\n",
    "    # converts List[STrack] into format that can be consumed by match_detections_with_tracks function\n",
    "    def tracks2boxes(tracks: List[STrack]) -> np.ndarray:\n",
    "        return np.array([\n",
    "            track.tlbr\n",
    "            for track\n",
    "            in tracks\n",
    "        ], dtype=float)  \n",
    "\n",
    "    @staticmethod\n",
    "    # matches our bounding boxes with predictions\n",
    "    def match_detections_with_tracks(\n",
    "        detections: Detections,\n",
    "        tracks: List[STrack]\n",
    "    ) -> List[int]:\n",
    "        \n",
    "        # check if there is a detections\n",
    "        if not np.any(detections.xyxy) or len(tracks) == 0:\n",
    "            return [None] * len(detections.xyxy)\n",
    "\n",
    "        tracks_boxes = ObjectDetector.tracks2boxes(tracks=tracks) # Converts the tracks into bounding box format using the tracks2boxes function.\n",
    "        iou = box_iou_batch(tracks_boxes, detections.xyxy) # obtaining iou using the box_iou_batch method\n",
    "        track2detection = np.argmax(iou, axis=1) # Finds the index of the detection (from detections) that has the highest IOU with each track, indicating the best match.\n",
    "\n",
    "        # Creating a list where we will store the track ids\n",
    "        tracker_ids = [None] * len(detections.xyxy)\n",
    "\n",
    "        # assign track ids to each detected object that meets the iou threshold\n",
    "        for tracker_index, detection_index in enumerate(track2detection):\n",
    "            if iou[tracker_index, detection_index] != 0:\n",
    "                tracker_ids[detection_index] = tracks[tracker_index].track_id\n",
    "\n",
    "        return tracker_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27fc85b-0606-492b-8995-d1c730f112d9",
   "metadata": {},
   "source": [
    "## Main Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0f58ec-4493-4e0e-82af-b45450090927",
   "metadata": {},
   "source": [
    "The main function ties everything together that we built. From detection to tracking and counting of detected objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215d49d8-b4ed-4121-996e-a9fd5d12bd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Initialize the detector and camera\n",
    "    detector = ObjectDetector(config) # we will create an instance of the ObjectDetector class \n",
    "    cap = initialize_camera(config) # initialize camera using the initialize_camera function\n",
    "\n",
    "    try:\n",
    "        while True: # it will loop infinitely to capture and detect images until it breaks or exception occurs\n",
    "            ret, frame = cap.read() # it will read the image that the camera captures\n",
    "            if not ret: # if the ret or return in the capturing reading is false then it will break\n",
    "                logger.error(\"Could not read frame.\") \n",
    "                break\n",
    "            # if there is a frame captured, it will pass to the instance method of process_frame to get the bounding boxes drawn and a count of detected objects by class. \n",
    "            processed_frame, class_counts = detector.process_frame(frame)\n",
    "            # then it will output to the window the processed frame\n",
    "            cv2.imshow(\"YOLOv7 Live\", processed_frame)\n",
    "            print(f\"{inference_time:.4f} ms to do a forward inference time per image.\") # print in the terminal the inference time per image\n",
    "            # This waits for 30 milliseconds for a key press. If the Esc keyis pressed, it breaks the loop.\n",
    "            if cv2.waitKey(30) == 27:  # ASCII of Esc key is 27\n",
    "                break\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"An error occurred: {e}\") # If any exception occurs during the process, it's caught and logged.\n",
    "    finally: # then we will release all of the resources\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
